# Multimodal-survey
investigate the relevant works in the multi-model area

<b>xxxx-xx-xx</b><br>
<i>Title</i>: <a href="https://arxiv.org/pdf/2004.15015.pdf">xxx</a> (arXiv 2020)<br>
<i>Author</i>: xxx<br>
<i>Comments</i>: 在xxx领域，解决xxx科研问题。使用了xxx方法，获得xxx效果。有什么启发。
</br>

<b>2022-10-16</b><br>
<i>Title</i>: <a href="[https://arxiv.org/pdf/2004.15015.pdf](https://www.isca-speech.org/archive/pdfs/interspeech_2021/wang21ga_interspeech.pdf)">Learning Mutual Correlation in Multimodal Transformer for Speech Emotion Recognition</a> (interspeech2021)<br>
<i>Author</i>: Yuhua Wang, Guang Shen, Yuezhu Xu, Jiahang Li, Zhengdao Zhao<br>
<i>Comments</i>: 在情感识别领域，不同模态高维时间特征之间的关系还有待研究。提出了在每一个transformer层都share参数的模型，在IEMOCAP用WA和UA指标获得SOTA效果。
我觉得科研问题很直白，做了个复杂的模型。然后结果并不solid，指标的普适性也待考察。所以投short可以理解。
</br>


<b>2022-10-17</b><br>
<i>Title</i>: <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/ito21_interspeech.pdf">Audio-Visual Speech Emotion Recognition by Disentangling Emotion and Identity Attributes</a> (interspeech2021)<br>
<i>Author</i>: Koichiro Ito, Takuya Fujioka, Qinghua Sun, and Kenji Nagamatsu<br>
<i>Comments</i>: 在情感识别领域，作者认为有一些非情感的特征会影响情感识别的结果。于是提出了通过分离出只抽取情感识别特征和身份特征的方法，最终稍稍超过sota结果。
</br>

<i>Title</i>: <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/ito21_interspeech.pdf">Audio-Visual Speech Emotion Recognition by Disentangling Emotion and Identity Attributes</a> (interspeech2021)<br>
<i>Author</i>: Koichiro Ito, Takuya Fujioka, Qinghua Sun, and Kenji Nagamatsu<br>
<i>Comments</i>: 在情感识别领域，作者认为有一些非情感的特征会影响情感识别的结果。于是提出了通过分离出只抽取情感识别特征和身份特征的方法，最终稍稍超过sota结果。
</br>
